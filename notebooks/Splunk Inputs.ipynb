{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d55ac4-b014-412f-b08e-a474f3ccd44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in merged_sessions.csv is: 17701957\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress DtypeWarnings for a cleaner output\n",
    "warnings.filterwarnings('ignore', category=pd.errors.DtypeWarning)\n",
    "\n",
    "\n",
    "# Load the merged sessions file\n",
    "# Note: You may need to use the chunking method if the file is too large\n",
    "try:\n",
    "    merged_sessions_df = pd.read_csv('../outputs/merged_sessions.csv')\n",
    "    num_rows = merged_sessions_df.shape[0]\n",
    "    print(f\"The number of rows in merged_sessions.csv is: {num_rows}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"The file might be too large. Try reading it in chunks to get the row count.\")\n",
    "    # Fallback to a chunking method for large files\n",
    "    chunk_size = 10000\n",
    "    total_rows = 0\n",
    "    for chunk in pd.read_csv('outputs/merged_sessions.csv', chunksize=chunk_size):\n",
    "        total_rows += len(chunk)\n",
    "    print(f\"The number of rows (using chunking) is: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118162a0-b87c-4023-b92f-2ea0e26061ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 to ../splunk_inputs/email_chunks\\email_chunk_1.csv\n",
      "Saved chunk 2 to ../splunk_inputs/email_chunks\\email_chunk_2.csv\n",
      "Saved chunk 3 to ../splunk_inputs/email_chunks\\email_chunk_3.csv\n",
      "Saved chunk 4 to ../splunk_inputs/email_chunks\\email_chunk_4.csv\n",
      "Saved chunk 5 to ../splunk_inputs/email_chunks\\email_chunk_5.csv\n",
      "Saved chunk 6 to ../splunk_inputs/email_chunks\\email_chunk_6.csv\n",
      "Saved chunk 7 to ../splunk_inputs/email_chunks\\email_chunk_7.csv\n",
      "Saved chunk 8 to ../splunk_inputs/email_chunks\\email_chunk_8.csv\n",
      "Saved chunk 9 to ../splunk_inputs/email_chunks\\email_chunk_9.csv\n",
      "Saved chunk 10 to ../splunk_inputs/email_chunks\\email_chunk_10.csv\n",
      "Saved chunk 11 to ../splunk_inputs/email_chunks\\email_chunk_11.csv\n",
      "Saved chunk 12 to ../splunk_inputs/email_chunks\\email_chunk_12.csv\n",
      "Saved chunk 13 to ../splunk_inputs/email_chunks\\email_chunk_13.csv\n",
      "Saved chunk 14 to ../splunk_inputs/email_chunks\\email_chunk_14.csv\n",
      "Saved chunk 15 to ../splunk_inputs/email_chunks\\email_chunk_15.csv\n",
      "Saved chunk 16 to ../splunk_inputs/email_chunks\\email_chunk_16.csv\n",
      "Saved chunk 17 to ../splunk_inputs/email_chunks\\email_chunk_17.csv\n",
      "Saved chunk 18 to ../splunk_inputs/email_chunks\\email_chunk_18.csv\n",
      "Saved chunk 19 to ../splunk_inputs/email_chunks\\email_chunk_19.csv\n",
      "Saved chunk 20 to ../splunk_inputs/email_chunks\\email_chunk_20.csv\n",
      "Saved chunk 21 to ../splunk_inputs/email_chunks\\email_chunk_21.csv\n",
      "Saved chunk 22 to ../splunk_inputs/email_chunks\\email_chunk_22.csv\n",
      "All chunks have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the path to your large CSV file\n",
    "large_file_path = '../data/email.csv' \n",
    "# Set the path to the directory where you want to save the smaller files\n",
    "output_dir = '../splunk_inputs/email_chunks' \n",
    "# Define the size of each chunk (in rows)\n",
    "# 500,000 rows should produce a file well under the 500 MB limit\n",
    "chunk_size = 500000 \n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# --- Splitting the file ---\n",
    "chunk_number = 0\n",
    "for chunk in pd.read_csv(large_file_path, chunksize=chunk_size):\n",
    "    chunk_number += 1\n",
    "    # Define the output file name\n",
    "    output_file_name = f'email_chunk_{chunk_number}.csv'\n",
    "    output_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the chunk to a new CSV file\n",
    "    chunk.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved chunk {chunk_number} to {output_path}\")\n",
    "\n",
    "print(\"All chunks have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59fe962-ca6a-40ce-8acd-8591e08c6f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 to ../splunk_inputs/file_chunks\\email_chunk_1.csv\n",
      "Saved chunk 2 to ../splunk_inputs/file_chunks\\email_chunk_2.csv\n",
      "Saved chunk 3 to ../splunk_inputs/file_chunks\\email_chunk_3.csv\n",
      "Saved chunk 4 to ../splunk_inputs/file_chunks\\email_chunk_4.csv\n",
      "Saved chunk 5 to ../splunk_inputs/file_chunks\\email_chunk_5.csv\n",
      "All chunks have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set the path to your large CSV file\n",
    "large_file_path = '../data/file.csv' \n",
    "# Set the path to the directory where you want to save the smaller files\n",
    "output_dir = '../splunk_inputs/file_chunks' \n",
    "# Define the size of each chunk (in rows)\n",
    "# 500,000 rows should produce a file well under the 500 MB limit\n",
    "chunk_size = 500000 \n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# --- Splitting the file ---\n",
    "chunk_number = 0\n",
    "for chunk in pd.read_csv(large_file_path, chunksize=chunk_size):\n",
    "    chunk_number += 1\n",
    "    # Define the output file name\n",
    "    output_file_name = f'email_chunk_{chunk_number}.csv'\n",
    "    output_path = os.path.join(output_dir, output_file_name)\n",
    "    \n",
    "    # Save the chunk to a new CSV file\n",
    "    chunk.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved chunk {chunk_number} to {output_path}\")\n",
    "\n",
    "print(\"All chunks have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d798fd8b-b271-44c5-98ff-dd1d78ba8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have the email.csv file loaded as 'email_df'\n",
    "email = pd.read_csv('../data/email.csv')\n",
    "\n",
    "# Extract the domain from the 'from' email addresses\n",
    "email['domain'] = email['from'].apply(lambda x: x.split('@')[1])\n",
    "\n",
    "# Count the frequency of each domain\n",
    "domain_counts = email['domain'].value_counts()\n",
    "\n",
    "print(\"Top 10 most common domains:\")\n",
    "print(domain_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f341d67-1283-4751-af5a-62376f620d59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
