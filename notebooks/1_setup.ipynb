{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52090193-e05b-41d6-8645-dc9d90ce47b1",
   "metadata": {},
   "source": [
    "# Phase 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3da69b-d2f0-41fa-b7bb-3cf7e0223ec7",
   "metadata": {},
   "source": [
    "## Import Libraries and Define File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbdfae0-1360-4aee-81a6-e17488cc1ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and file paths defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualisation libraries (to be used later)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries (to be used later)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Path to your data files\n",
    "data_path = '../data/'\n",
    "logon_file = data_path + 'logon.csv'\n",
    "file_file = data_path + 'file.csv'\n",
    "email_file = data_path + 'email.csv'\n",
    "device_file = data_path + 'device.csv'\n",
    "psychometric_file = data_path + 'psychometric.csv'\n",
    "\n",
    "print(\"Libraries imported and file paths defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d932952-0708-4a63-a892-6e483f68d9a0",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9d28fb-7255-4e41-bee9-95b45f5aec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully.\n",
      "Logon DataFrame shape: (3530285, 5)\n",
      "File DataFrame shape: (2014883, 9)\n",
      "Email DataFrame shape: (10994957, 12)\n",
      "Device DataFrame shape: (1551828, 6)\n",
      "Psychometric DataFrame shape: (4000, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "logon = pd.read_csv(logon_file)\n",
    "file= pd.read_csv(file_file)\n",
    "email = pd.read_csv(email_file)\n",
    "device= pd.read_csv(device_file)\n",
    "psychometric = pd.read_csv(psychometric_file)\n",
    "\n",
    "print(\"Datasets loaded successfully.\")\n",
    "print(\"Logon DataFrame shape:\", logon.shape)\n",
    "print(\"File DataFrame shape:\", file.shape)\n",
    "print(\"Email DataFrame shape:\", email.shape)\n",
    "print(\"Device DataFrame shape:\", device.shape)\n",
    "print(\"Psychometric DataFrame shape:\", psychometric.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52987585-45b2-4a69-916d-da852c8697ea",
   "metadata": {},
   "source": [
    "## Timestamps Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210691e0-2a0c-406b-afcd-ab7067a53c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp columns converted to datetime.\n",
      "Example of converted timestamp: 2010-01-02 02:19:18\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamps to datetime format\n",
    "logon['date'] = pd.to_datetime(logon['date'])\n",
    "file['date'] = pd.to_datetime(file['date'])\n",
    "email['date'] = pd.to_datetime(email['date'])\n",
    "device['date'] = pd.to_datetime(device['date'])\n",
    "\n",
    "print(\"Timestamp columns converted to datetime.\")\n",
    "print(\"Example of converted timestamp:\", logon['date'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bdbf56-eb70-48c4-8eca-bbad2c8a52c4",
   "metadata": {},
   "source": [
    "## Datasets Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa258e6-5925-4c31-8fd2-059447305ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets merged using the master index.\n",
      "Merged DataFrame shape: (17701957, 22)\n"
     ]
    }
   ],
   "source": [
    "# Create a unique master index of all user-date pairs\n",
    "all_user_dates = pd.concat([logon[['user', 'date']], \n",
    "                           file[['user', 'date']], \n",
    "                           email[['user', 'date']],\n",
    "                           device[['user', 'date']]]).drop_duplicates()\n",
    "\n",
    "# Use this master index to create a new, empty DataFrame\n",
    "master_df = all_user_dates.copy()\n",
    "\n",
    "# Iteratively merge the features from each log file\n",
    "master_df = pd.merge(master_df, logon.drop_duplicates(subset=['user', 'date']), on=['user', 'date'], how='left')\n",
    "master_df = pd.merge(master_df, file.drop_duplicates(subset=['user', 'date']), on=['user', 'date'], how='left')\n",
    "merged = pd.merge(master_df, email.drop_duplicates(subset=['user', 'date']), on=['user', 'date'], how='left')\n",
    "\n",
    "print(\"All datasets merged using the master index.\")\n",
    "print(\"Merged DataFrame shape:\", merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5081d697-a554-42f0-96b6-e93f6ec3c268",
   "metadata": {},
   "source": [
    "## Handle Missing Values and Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0c29361-9e1d-4614-acfe-6fbc12596d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values filled with 0.\n",
      "Duplicate rows dropped.\n",
      "Final Merged DataFrame shape: (17701957, 22)\n"
     ]
    }
   ],
   "source": [
    "# Fill missing numerical values with 0, as NaN often means no activity occurred\n",
    "# You can select specific columns or apply to all numerical ones.\n",
    "merged = merged.fillna(0)\n",
    "\n",
    "# Drop any potential duplicate rows\n",
    "merged.drop_duplicates(inplace=True)\n",
    "\n",
    "print(\"Missing values filled with 0.\")\n",
    "print(\"Duplicate rows dropped.\")\n",
    "print(\"Final Merged DataFrame shape:\", merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fadb943-c702-4fe4-8b02-731ba570c2af",
   "metadata": {},
   "source": [
    "## Save the Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71423bac-0f86-4e4b-a2be-019b79cd22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged dataset saved to './outputs/merged_sessions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Create an 'outputs' directory if it doesn't exist\n",
    "import os\n",
    "if not os.path.exists('../outputs'):\n",
    "    os.makedirs('../outputs')\n",
    "\n",
    "# Save the final dataset\n",
    "merged.to_csv('../outputs/merged_sessions.csv', index=False)\n",
    "\n",
    "print(\"Final merged dataset saved to '../outputs/merged_sessions.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50b064-d08a-40aa-b6e6-ebd98e282fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
